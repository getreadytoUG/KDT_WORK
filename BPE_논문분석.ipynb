{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/getreadytoUG/KDT_WORK/blob/main/BPE_%E1%84%82%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 논문을 이해하는데 알아두면 좋은 개념들\n",
        "* 통계 기반 기계번역(SMT)\n",
        "    * 병렬 말뭉치를 기반으로 번역 모델을 학습.\n",
        "> 병렬 말뭉치란?  \n",
        "> 영어-프랑스어 번역 말뭉치, 영어-한국어 번역 말뭉치\n",
        "    * 각 단어 또는 구의 번역 확률을 추정하여 학습함.\n",
        "    * 학습된 모델은 확률을 기반으로 최적의 번역을 찾아냄.\n",
        "    * ML기법이라고 생각\n",
        "\n",
        "* 구문 기반 기계번역(PBMT)\n",
        "    * 병렬 텍스트 쌍을 사용하여 학습.\n",
        "    * 각 구와 번역된 구의 대응 관계가 훈련됨.\n",
        "    * SMT의 기법중 하나\n",
        "\n",
        "* 문자 기반 기계번역? -> 서브워드 유닛을 사용한 번역 기법이라고 생각하면 됨.\n",
        "    * 서브워드 유닛은 인공신경망 기계 번역에만 사용되는 것은 아님.\n",
        "    * 따라서 문자 기반이라고 해도 여러가지 방법이 있다...\n",
        "\n",
        "* 어텐션 메커니즘\n",
        "    * 입력 시퀀스의 특정 부분에 '집중'하게 한다.\n",
        "    * 현재 문장에서 번역되는 어떤  부분이 원문의 어떤 단어에 주로 의존하는지 동적으로 결정한다.\n",
        "    * soft attention, absolute position attention, multi-head attention 등이 있음\n",
        "\n",
        "* 가변 길이 표현\n",
        "    * 가변 길이 입력에 대해 고정적인 벡터를 만드는 과정에서 생기는 문제가 있음(고정 길이 표현의 정보 병목 문제)\n",
        "    * 길이의 차이가 많아도 고정적인 벡터로 표현을 할 수 있어야함. 그러기엔 아무래도 한계가 있음.\n",
        "    * 어텐션 메커니즘 등을 사용하면 고정적인 벡터여도 가변 길이 입력을 잘 표현할 수 있음.\n",
        "\n",
        "* 텍스트 압축률\n",
        "    * 입력 데이터를 토큰화를 잘 하던지 문장의 필요없는 부분을 제외한다던지 해서 크기를 줄이는 것?\n",
        "    * 토큰화, 서브워드 유닛 적용 이후의 표현이 원본 텍스트에 비해 더 간결한 정도를 의미함."
      ],
      "metadata": {
        "id": "hmCnWAajXyP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OI83ebJbmW9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-1. Related Work\n",
        "* SMT 즉, 통계 기반 기계 번역은 모르는 단어에 대한 처리가 힘들다\n",
        "\n",
        "* SMT에선 '이름(고유명사)'같이 어원이 불명확한 단어가 모르는 단어의 대부분이라고 한다.\n",
        "> ex) Tesla를 독일어로 번역해도 Tesla라고 표기함.\n",
        "\n",
        "* 위의 경우는 알파벳을 공유하기 때문에 그대로 복사해도 된다. 하지만 한글의 경우 '테슬라'로 번역될 것이다.\n",
        "\n",
        "* 문자 기반 번역은 구문 기반 모델(PBM)과 함께 연구되었고, 밀접한 관련 언어(한국어-일본어)에 대해 효과적이다.\n",
        "> 즉, 문자 기반 번역이란 애초에 효과적인 방법이다?\n",
        "\n",
        "* 합성어를 쪼개는 방법(알고리즘이 다양함)도 SMT에서 사용되어왔다. -> NMT에서 더 효과적인?\n",
        "\n",
        "* PBMT는 분할 규칙이 보수적임 -> 좀 더 진보적인 분할 규칙을 사용하는 것이 목표임. 물론 해냄\n",
        "> 그 규칙은 곧.. BPE, WPT등에 사용되는 서브워딩 기법을 보면 이해됨. (OOV해결을 위한)\n",
        "\n",
        "* 서브워딩은 작업마다 다름, 음절로 나누기, 음성으로 나누기 등등..\n",
        "\n",
        "* 다양한 나라의 언어를 쪼갤때는 일반화된 알고리즘을 사용하도록 제안됨. 하나의 나라에 국한되면 좋진 않으니까?\n",
        "\n",
        "* 단어 기반 접근 방식과 BPE, WPT 방식은 다른 것이다.\n",
        "단어 기반 접근 방식과 문자 기반 접근 방식은 다르다.\n",
        "\n",
        "* 어텐션 메커니즘이 어떤 수준에서 작동하는지(단어, 문자, 구)\n",
        "\n",
        "* 서브워드 단위 방식은 고정 길이 표현의 정보 병목을 막을 수 있다.\n",
        "\n",
        "* NMT는 PBMT와 다르게 시간 및 공간 효율성을 높이고 백오프 모델이 필요없도록 어휘 크기를 최소화함.\n",
        "\n",
        "* 텍스트 자체를 컴팩트하게 표현하는 것도 원한다.\n",
        "\n",
        "* BPE방식을 사용하여 압축률이 높은 어휘를 학습할 수 있다.\n"
      ],
      "metadata": {
        "id": "Vg0jGKQBbKG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-2. Byte Pair Encoding\n",
        "\n",
        "* 데이터 압축 기술중 하나임.\n",
        "\n",
        "* 바이트 쌍을 병합함.\n",
        "\n",
        "* 심볼 어휘로 문자를 사용함(바이트 단위)\n",
        "\n",
        "* 각 단어를 심볼 어휘 즉, 바이트 시퀀스로 나타냄. 단어 끝엔 '.'을 추가하여 단어의 끝임을 알림\n",
        "\n",
        "* 반복적으로 모든 쌍을 세고 가장 빈도가 높은 쌍을 하나의 심볼로 대체함.\n",
        "\n",
        "* 별도의 사전 목록이 필요가 없다. 빈번한 문자나 n-gram이 결국 하나의 심볼이 되기 때문이다.\n",
        "\n",
        "* 최종 심볼 어휘의 수 = (초기 심볼 수) + (병합 횟수)\n",
        "> 초기 심볼은 바이트 단위\n",
        "\n",
        "* 각 단어는 독립적으로 토큰화가 진행됨. 입력 텍스트에서 추출한 단어 딕셔너리에서 동작함. (value는 단어의 빈도수)\n",
        "\n",
        "아래는 코드 예시임\n"
      ],
      "metadata": {
        "id": "mD-_tNH5WKlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, collections\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2, 'n e w e s t </w>':6,'w i d e s t </w>':3}\n",
        "num_merges = 10\n",
        "\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    # key=paris.get은 딕셔너리의 value를 기준으로 삼겠다는 의미\n",
        "    # 빈도수가 같으면 딕셔너리중 먼저 나온 값이 반환됨.\n",
        "    # 이 순서로 결과가 바뀔 수도 있기는 할 것이여!\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print(f'pairs: {pairs}')\n",
        "    print(f'bset: {best}\\n')\n",
        "\n",
        "print(f'최종 vocab: {vocab}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GYFVOU9p0X4",
        "outputId": "e40dba5e-5868-4adc-90b1-16b174735e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pairs: defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 8, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3})\n",
            "bset: ('e', 's')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'es'): 6, ('es', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'es'): 3})\n",
            "bset: ('es', 't')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est'): 6, ('est', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3})\n",
            "bset: ('est', '</w>')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('l', 'o')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('lo', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('lo', 'w')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('n', 'e')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('ne', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('ne', 'w')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('new', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('new', 'est</w>')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('low', '</w>')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('w', 'i')\n",
            "\n",
            "최종 vocab: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 이 논문에서는 BPE를 사용하며, 모든 쌍을 인덱싱하고 데이터 구조를 점진적으로 업데이트함으로써 효율을 높였다고 한다.\n",
        "* 즉, 이 코드 자체가 BPE는 아님, BPE 기법을 사용하여 단어를 문자단위부터 점차적으로 합쳐나가며 토큰화에 사용될 하나의 심볼들로 만든 것임.\n"
      ],
      "metadata": {
        "id": "f8bx7b8kp1t9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 가변 길이 인코딩을 수행하기 위한 다른 압축 알고리즘과의 차이점이 있다.\n",
        "\n",
        "* 심볼 시퀀스가 계속하여 서브워드 단위로 해석이 가능하고, 이러한 기법을 기반으로 훈련 시에 보지 못한 새로운 단어를 번역할 수 있게 한다.\n",
        "\n",
        "* 분절된 바이트단위 문자는 더 큰, 존재할법한? 심볼로 병합이 된다.\n",
        "\n",
        "* 예시로 {‘low’, ‘lowest’, ‘newer’, ‘wider’} 을 학습하고 'lower'를 입력했다 하자. 위 코드의 결과인 심볼 시퀀스에 'low', 'er'이 있기 때문에 해당 입력은 'low+er'로 분절화가 된다.\n",
        "\n"
      ],
      "metadata": {
        "id": "rrXDJ2ZF7B_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 이 논문에선 소스 데이터를 위한 BPE, 타겟 데이터를 위한 BPE가 독립적인 경우와 joint BPE의 경우를 모두 다룬다.\n",
        "\n",
        "* Learning two independent encodings (source and target vocabulary):\n",
        "\n",
        "    * 소스 언어와 타겟 언어 각각에 대해 독립적인 BPE 인코딩을 학습합니다.\n",
        "    * 각 언어에 대해 훈련된 텍스트를 기반으로 각각의 언어에 특화된 서브워드를 생성합니다.\n",
        "    * 두 언어 간에 서로 다른 토크나이징이 가능하며, 텍스트와 어휘 크기 측면에서 각각의 언어에 특화된 효율성을 제공합니다.\n",
        "\n",
        "* Learning the encoding on the union of the two vocabularies (joint BPE):\n",
        "\n",
        "    * 소스 언어와 타겟 언어의 어휘를 합쳐 하나의 BPE 인코딩을 학습합니다.\n",
        "    * 두 언어 간에 서로 다른 토크나이징을 사용하지 않고, 하나의 통일된 서브워드 토크나이징을 적용합니다.\n",
        "    * 언어 간에 토크나이징 일관성을 유지하며, 텍스트 및 어휘 크기 측면에서 효율적입니다.\n",
        "\n",
        "* BPE를 소스, 타겟에 독립적으로 적용하면 같은 이름이 다르게 분절될 수 있음. 이는 매핑 문제로 이어짐.\n",
        "\n",
        "* 두 언어간 텍스트 일관성을 높이기 위해 사용하는 방법이 있음\n",
        "    \n",
        "    * 러시아어 -> ISO-9 사용(국제 표준) -> 라틴 문자\n",
        "    * BPE 적용\n",
        "    * 결과를 키릴 문자로 표기하면 러시아어 원래 표기로 돌아감\n"
      ],
      "metadata": {
        "id": "5SV_jxU8-5eZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Evaluation(평가)\n"
      ],
      "metadata": {
        "id": "6dBLVZzQ5YgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> 알아두면 좋은 추가정보\n",
        "\n",
        "* WMT(Workshop on Machine Translation): 2015의 공유 번역 과제는 기계 번역에 관한 워크샵에서 제공한 데이터를 기반으로 한 번역 실험을 의미합니다. WMT는 기계 번역 연구 및 개발을 위한 국제 워크샵으로, 매년 번역 시스템을 평가하기 위한 공동 번역 과제를 제공합니다. 2015년 WMT의 공유된 번역 과제는 그 해에 제공된 데이터를 사용하여 번역 모델을 평가하고 비교하는 데 사용되었습니다.\n",
        "\n",
        "* truecasing: 텍스트에서 대문자와 소문자를 올바르게 구분하여 적용하는 작업.\n",
        "\n",
        "* [moses](https://www2.statmt.org/moses/): Moses는 기계 번역을 위한 오픈 소스 툴킷 중 하나로, 대표적인 구현 중 하나입니다. Moses는 통계적 기계 번역 (Statistical Machine Translation, SMT) 시스템을 구축하고 사용하기 위한 다양한 도구와 라이브러리를 제공합니다. 주로 명령 줄 인터페이스를 통해 상호 작용하며, 다양한 작업에 사용됩니다.\n",
        "\n",
        "* BELU(Bilingual Evaluation Understudy): 기계 번역 결과를 평가하는 데 사용되는 자동 평가 지표 중 하나. BLEU는 먼저 인간 평가자들이 번역을 평가한 결과를 참조(reference)로 사용하고, 이를 기반으로 자동적으로 번역 결과의 품질을 측정합니다.\n",
        "\n",
        "* CHRF3: CHRF3는 BLEU와 마찬가지로 기계 번역 결과를 평가하는 데 사용되는 자동 평가 지표 중 하나입니다. Popovic ́에 의해 제안된 이 지표는 번역 결과의 품질을 측정하는 데에 있어 BLEU보다 더 효과적인 결과를 내는 경우가 있습니다.\n",
        "\n",
        "* CHRF3는 문자 n-그램 F3 점수로, 특히 영어를 기반으로 한 번역에서 인간 판단과 잘 관련되어 있다고 발견되었습니다.\n",
        "\n",
        "* unigram F1: 모델이 생성한 텍스트와 실제 정답 텍스트 간의 유사성을 측정하는 데에 사용됨. 정밀도와 재현율의 조화평균을 사용하여 성능을 평가함.\n",
        "\n",
        "* word alignment\n",
        "\n",
        "```\n",
        "[\"나는\", \"너를\", \"사랑해\"]\n",
        "[\"I\", \"love\", \"you\"]\n",
        "```\n",
        "각 언어가 서로 대응되는 정보를 담고 있는 것.\n",
        "\n",
        "대표적으로 Pharaoh format이 있음(fast align도 이 방식임)\n",
        "\n",
        "```\n",
        "한국어 to 영어\n",
        "\n",
        "0-0 1-2 2-1  \n",
        "```\n",
        "\n",
        "```\n",
        "영어 to 한국어\n",
        "\n",
        "0-0 2-1 1-2\n",
        "```"
      ],
      "metadata": {
        "id": "7qJvMl1vCYYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NMT에서 이전에 보지 못한 단어들의 번역을 서브워드 단위로 표현함으로써 개선할 수 있을까?  \n",
        "\n",
        "> 단어 크기, 텍스트 크기 및 번역 품질 측면에서 어떤 서브워드 단위의 분절이 가장 우수한 성능을 보일까?"
      ],
      "metadata": {
        "id": "SlRFKFXmBM_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WMT의 공유 과제에 대한 내용이 나옴\n",
        "\n",
        "* 영어 -> 독일어, train_set: 약 4.2백만개 문장 쌍 or 약 1억개의 토큰\n",
        "\n",
        "* 영어 -> 러시아어, train_set: 약 2.6백만개 문장 쌍 or 약 5천만개의 토큰\n",
        "\n",
        "* 사용한 개발 셋\n",
        "    * newstest2013를 개발용으로 사용\n",
        "    * newstest2014, 2015에 제출하여 점수를 얻었다?\n",
        "\n",
        "* 사용한 평가 지표\n",
        "    * BELU, CHRF3\n",
        "\n",
        "* 모르는 단어의 번역과 관련이 있기 때문에 이에 대한 통계를 보여줄 것임. 그 통계는 유니그램 F1을 통해 측정함.\n",
        "\n",
        "* 모든 실험은 Groundghog를 사용했음. https://github.com/sebastien-j/LV_groundhog/blob/master/experiments/nmt/README.md\n",
        "\n"
      ],
      "metadata": {
        "id": "QZJc-c9zCDB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 사용된 세팅\n",
        "\n",
        "* hidden layer 1000\n",
        "\n",
        "* embedding layer 620\n",
        "\n",
        "* shortlist 30000\n",
        "\n"
      ],
      "metadata": {
        "id": "ncEJN39mLy2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 세팅\n",
        "\n",
        "* Adadelta\n",
        "    * minibatch=80, reshuffle between epochs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WiN5yvzgMJXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 약 7일동안 4개의 최종 모델을 만들고 고정된 임베딩 층으로 12시간동안 학습을 시킴.\n",
        "\n",
        "* 각 모델(최종 4개)에 대해 cut-off gradient clipping을 한 번은 5.0, 한 번은 1.0으로 설정. 1.0일 때가 더 좋았음.\n",
        "\n",
        "* newstest2013에서 가장 우수한 성능을 보인 시스템을 보고함\n",
        "\n",
        "* 총 8개의 모델에 대해서도 보고함\n",
        "\n",
        "* 문장 길이에 따라 정규화된 확률을 사용, 빔 탐색크기는 12.\n",
        "\n",
        "* fast-align을 사용함. 이는 희귀한 단어의 백오프 사전으로 사용됨.\n",
        "\n",
        "* 실험에서 번역 속도를 높이기 위해 딕셔너리를 사용함.\n",
        "\n",
        "* 후보 번역 목록에 대한 softmax를 수행할 때 필터링된 목록만 사용함."
      ],
      "metadata": {
        "id": "Pu3HO2PKMc4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-1. Subword statistics\n",
        "\n",
        "* 실험을 통해 확인할 번역 품질 외에도 주요 목표는 콤팩트하고 고정 크기의 서브워드 어휘를 통해 open-vocabulary를 표현하고 효율적인 훈력 및 디코딩을 가능하게 하는 것이다.\n",
        "\n",
        "* 표1을 참고\n",
        "\n",
        "* n-gram에서 어떤 n값을 선택하느냐에 따라 어휘 크기와 시퀀스 길이 간의 균형을 조절할 수 있습니다.\n",
        "\n",
        "* 시퀀스 길이의 증가가 상당하다면 가장 빈번한 k개의 단어 유형을 분할하지 않은 상태로 두는 방법이 있다.\n",
        "\n",
        "* 유니그램 표현만이 Open-vocabulary이다... 하지만 예비 실험에서 성능이 좋지 않았다... 그래서 바이그램 표현으로 했다... 그러나 테스트 세트의 일부 토근은 훈련 세트로 생성할 수 없었다...\n",
        "\n",
        "* 이전 SMT 연구에서 유용하다고 알려진 여러 단어 분절 기술이 있다.\n",
        "    * 빈도 기반 복합 분리(Koehn and Knight, 2003)\n",
        "    * 규칙 기반 음절 나눔(Liang, 1983)\n",
        "    * Morfessor(Creutz and Lagus, 2002)\n",
        "\n",
        "* 이 기법들이 어휘 크기를 줄이긴 하지만 알려지지 않은 단어 문제는 해결하지 못했다. 백오프 사전 없으면 부적합하다.\n",
        "\n",
        "* BPE는 open-vocabulary 목표를 충족하고 알려지지 않은 단어 문제도 해결해줌.\n",
        "\n",
        "* BPE 최고, Table 1 보셈 딱 봐도 좋잖슴\n",
        "\n",
        "* 실제로는 드물게 나타나는 서브워드 단위는 제외했음."
      ],
      "metadata": {
        "id": "KOjkjFyxR_gc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-2. Translation experiments\n",
        "\n",
        "* WDict: 백오프 사전을 사용하는 단어 수준 모델\n",
        "* WUnk: 백오프 사전을 사용하지 않고, 없는 단어 UNK\n",
        "> 백오프 사전은 unigramF1을 향상시킨다. 하지만 이름을 표기하는 데 한정적이기 때문에 영어 -> 러시아어의 경우 향상이 작을 수 있다.\n",
        "* 모든 서브워드 시스템은 백오프 사전 없이 작동한다.\n",
        "\n",
        "* 모든 시스템이 기준 모델 대비 unigram F1 개선이 있었다. 특히 희귀한 단어의 경우\n",
        "\n",
        "```\n",
        "EN→DE의 경우 36.8% → 41.8%; EN→RU의 경우 26.5% → 29.7%\n",
        "\n",
        "```\n",
        "\n",
        "* OOV는 알파벳이 같은 경우(영어->독일어) 복사 전략은 잘 동작했다. 그러나 알파벳이 다른 경우(영어->러시아어)엔 서브워드 모델이 훨씬 결과가 좋았다.\n",
        "\n",
        "* 합친 BPE(BPE-J90k)가 가장 좋았음. 모든 서브워드 분할은 백오프 보다 우수하다.\n",
        "\n",
        "* BLEU와 CHRF3 지표의 불일치는 이 지표들 각각이 정밀도나 리콜과 같은 다른 편향을 가지고 있어서 발생\n",
        "\n",
        "* 암튼 성능 지표를 분석했음, NMT 성능 변동성은 여전히 열려 있는 문제라고 함."
      ],
      "metadata": {
        "id": "jzDcwKWxZOge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Analysis\n"
      ],
      "metadata": {
        "id": "UlUoWuYQjkl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-1. Unigram accuracy\n",
        "\n",
        "* 우리의 주요 주장은 단어 수준 NMT 모델에서 드문 및 알려지지 않은 단어의 번역이 부족하며, 서브워드 모델이 이러한 단어 유형의 번역을 개선한다는 것이다.\n",
        "\n",
        "* 빈도가 낮은 단어에 대한 Unigram F1은 항상 감소했다. 그런데 baseline시스템의 경우 OOV에 대한 F1이 좋았다. 그 이유는 '이름'을 복사하는 것이 효과적이기 때문이다.\n",
        "\n",
        "* 그냥 소스에서 복사된 이름의 경우를 제외하면 서브워드 시스템이 좋다.\n",
        "\n"
      ],
      "metadata": {
        "id": "181z4vEDjoVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"가장 빈도가 높은 50,000 단어에 대해서는 모든 신경망에서 표현이 동일하며, 이 범주에서 모든 신경망이 유사한 유니그램 F1을 달성합니다. 빈도 순위 50,000부터 500,000까지의 간격에서 C2-3/500k와 C2-50k 간의 비교에서 흥미로운 차이가 나타납니다. 두 시스템은 단일 유닛으로 이 구간의 단어를 나타내는 shortlist 크기만 다르며, C2-3/500k는 단일 유닛으로 나타내고, C2-50k는 서브워드 유닛을 통해 나타냅니다. C2-3/500k의 성능은 빈도 순위 500,000까지 심각하게 저하되며, 여기서 모델이 서브워드 표현으로 전환되고 성능이 회복됩니다. C2-50k의 성능은 더 안정적인 편입니다. 이는 서브워드 유닛이 단어보다 덜 희소하다는 사실 때문입니다. 우리의 훈련 세트에서 빈도 순위 50,000은 훈련 데이터에서 60의 빈도에 해당하며, 빈도 순위 500,000은 2의 빈도에 해당합니다. 서브워드 표현은 덜 희소하므로 신경망 어휘 크기를 줄이고 더 많은 단어를 서브워드 유닛을 통해 나타내면 성능이 향상될 수 있습니다.\""
      ],
      "metadata": {
        "id": "H2PL60HZjroE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"F1 숫자는 시스템 간의 일부 정성적인 차이를 숨기고 있습니다. 영어→독일어의 경우, WDict는 몇 개의 OOV(26.5% recall)를 생성하지만 높은 정밀도(60.6%)를 갖습니다. 반면, 서브워드 시스템은 높은 리콜을 달성하지만 낮은 정밀도를 갖습니다. 문자 바이그램 모델 C2-50k는 가장 많은 OOV 단어를 생성하며, 이 범주에 대해 상대적으로 낮은 정밀도(29.1%)를 달성합니다. 그러나 리콜(33.0%)에서는 백오프 사전을 능가합니다. BPE-60k는 분할 불일치로 인한 전사(또는 복사) 오류로 고통받아 정밀도(32.4%)가 약간 향상되었지만 리콜(26.6%)이 악화되었습니다. BPE-J90k의 경우 BPE-60k와 달리 분할 불일치로 인한 전사 오류를 개선하여 정밀도(38.6%)와 리콜(29.8%)이 모두 향상되었습니다.\""
      ],
      "metadata": {
        "id": "PWH-Se0Tlsx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 그런데 이제 500,000단어인 경우에 차이가 극명함.\n",
        "\n"
      ],
      "metadata": {
        "id": "7TMC67GlmNYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NMT란?Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem.\n",
        "\n",
        "\n",
        "Neural Machine Translation\n",
        "\n",
        "기존 NMT 모델은 고정된 어휘로 작동함.\n",
        "\n",
        "하지만 번역은 개방적 어휘 문제임.\n",
        "\n",
        "개방적 어휘 문제란? 새로운 단어에 대해 대응이 어렵다는 뜻\n",
        "\n",
        "Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary.\n",
        "\n",
        "기존의 연구에서는 OOV인 단어의 번역을 Dictionary에서 찾아서 함. 즉, 미리 구성된 사전에 등록된 단어나 표현을 사용함.\n",
        "\n",
        "In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units\n",
        "\n",
        "본 논문에서는 더 간단하고 효과적인 접근법을 소개함.\n",
        "\n",
        "그 효과적인 접근법이란 NMT모델이 희귀하거나 알려지지 않은 단어를 서브워드유닛 시퀀스를 통해 open-vocabulary 번역이 가능하도록 하는 방법이다.\n",
        "\n",
        "-> 서브워드 유닛을 사용하여 OOV를 해결하는 방법임\n",
        "\n",
        "This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations)\n",
        "\n",
        "\n",
        "다양한 단어분류들이 단어보다 작은 유닛들을 통해 번역이 가능하다는 직관을 기저로한다.\n",
        "\n",
        "예를들어 이름 (문자 복사 또는 표음법을 통해), 합성어 (구성적인 번역을 통해), 그리고 동형어 및 외래어(음운 및 형태론적 변형을 통해)\n",
        "\n",
        "해석:\n",
        "이 문장은 번역 모델이 단어보다 작은 언어 단위를 사용하여 다양한 종류의 단어를 처리할 수 있다는 개념을 설명하고 있습니다. 예를 들어, 이름은 문자를 그대로 복사하거나 발음에 따라 다른 문자로 변환함으로써 번역될 수 있습니다. 합성어는 각 구성 요소를 분해하고 구성적으로 해석하여 번역될 수 있습니다. 또한, 동형어와 외래어는 발음이나 형태론적 변형을 통해 번역될 수 있습니다. 이러한 접근은 단어 수준 이상의 작은 언어 단위를 고려하여 다양한 언어적 특징을 처리할 수 있는 모델을 의미합니다.\n",
        "\n",
        "\n",
        "We discuss the suitability of different word segmentation techniques, including simple character n- gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU, respectively.\n",
        "\n",
        "\n",
        "우리는 간단한 문자 n-gram 모델과 BPE를 사용한 세분화와 같은 기술의 적합함에 대해 생각하고, 경험적으로 보여주었다.\n",
        "\n",
        "\"서브워드 모델은 WMT 15 영어→독일어 및 영어→러시아어 번역 작업에서 백오프 사전 기준에 비해 최대 1.1 및 1.3 BLEU 점수 향상을 보입니다.\"\n",
        "\n",
        "해석: 우리가 생각해낸 서브워드 모델 사용 방법이 기존 방법에 비해 좋은 점수를 얻었다.\n",
        "\n",
        "\n",
        "서론\n",
        "\n",
        "\n",
        "Neural machine translation has recently shown impressive results. However, the translation of rare words is an open problem. The vocabulary of neural models is typically limited to 30000–50000 words, but translation is an open-vocabulary problem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level.\n",
        "\n",
        "\"최근에는 신경 기계 번역이 탁월한 결과를 보여주고 있습니다. 그러나 희귀한 단어의 번역은 여전히 열린 문제입니다. 신경 모델의 어휘는 일반적으로 30000~50000 단어로 제한되어 있지만, 번역은 개방 어휘 문제이며, 특히 응집 및 합성과 같은 생산적인 어휘 형성 과정이 있는 언어의 경우, 번역 모델은 단어 수준 이하의 메커니즘을 필요로 합니다.\"\n",
        "\n",
        "해석: NMT가 좋은 결과를 내고는 있는데, 번역은 아직 새로운 어휘에 대한 처리에 문제가 있다. 새로 만들어지는 합성어, 신조어 등의 언어는 단어수준 이하의 메커니즘이 필요하다.\n",
        "\n",
        "As an example, consider compounds such as the German Abwasser|behandlungs|anlange ‘sewage water treatment plant’, for which a segmented, variable-length representation is intuitively more appealing than encoding the word as a fixed-length vector.\n",
        "\n",
        "예를 들어, 독일어 \"Abwasserbehandlungsanlage\"와 같은 합성어를 고려해보겠습니다. 이 경우, 고정 길이 벡터로 단어를 인코딩하는 것보다 세분화되고 가변 길이의 표현이 직관적으로 더 매력적입니다.\n",
        "\n",
        "For word-level NMT models, the translation of out-of-vocabulary words has been addressed through a back-off to a dictionary look-up\n",
        "\n",
        "단어 수준의 NMT 모델에서는 어휘에 없는 단어의 번역이 사전 조회로 처리된다.\n",
        "\n",
        "\"Back-off\"은 일반적으로 어떤 전략이나 방법을 적용했을 때 원하는 결과가 나오지 않을 경우, 보다 간단하거나 일반적인 방법으로 되돌아가는 것을 의미합니다. 여기서 \"back-off to a dictionary look-up\"는 사전 조회에서 원하는 번역을 찾지 못할 때 간단한 사전 조회로 다시 돌아가는 전략을 나타냅니다. 따라서 실제로 사용되는 단어에 대한 번역이 어려울 때 미리 구성된 사전을 이용하여 대체하는 방법을 의미합니다.\n",
        "\n",
        "We note that such techniques make assumptions that often do not hold true in practice\n",
        "\n",
        "이러한 방법들은 실제에선 잘 안됨.\n",
        "\n",
        "For instance, there is not always a 1-to-1 correspondence between source and target words because of variance in the degree of morphological synthesis between languages, like in our introductory compounding example\n",
        "\n",
        "예를 들어, 언어 간 형태론적 합성의 정도에 변이가 있기 때문에 소스와 타겟 단어 간에 항상 일대일 대응이 있는 것은 아닙니다. 우리의 예시에서처럼 소개된 합성 예제에서도 이러한 상황이 발생할 수 있습니다.\n",
        "\n",
        "Also, word-level models are unable to translate or generate unseen words.\n",
        "\n",
        "또한, 단어 단위의 모델은 본 적 없는 단어를 번역하거나 생성할 줄 모른다.\n",
        "\n",
        "Copying unknown words into the target text is a reasonable strategy for names, but morphological changes and transliteration is often required, especially if alphabets differ\n",
        "\n",
        "알려지지 않은 단어를 타겟으로 복사하는 것은 이름에 대한 합리적인 전략이지만, 특히 알파벳이 다를 경우 형태론적 변화와 표음법이 종종 필요합니다\n",
        "\n",
        "We investigate NMT models that operate on the level of subword units\n",
        "\n",
        "우리는 서브워드 단위에서 작동하는 NMT 모델을 조사합니다.\n",
        "\n",
        "Our main goal is to model open-vocabulary translation in the NMT network itself, without requiring a back-off model for rare words.\n",
        "\n",
        "우리의 목표는 NMT 네트워크 자체에서 open-vocavulary를 모델링하는 것이며, 희귀한 단어에 대해 백오프 모델이 필요하지 않도록 하는 것이다.\n",
        "\n",
        "In addition to making the translation pro- cess simpler, we also find that the subword models achieve better accuracy for the translation of rare words than large-vocabulary models and back-off dictionaries, and are able to productively generate new words that were not seen at training time\n",
        "\n",
        "번역 과정을 더 간단하게 만드는 것 외에도, 서브워드 모델은 큰 어휘 모델과 백오프 사전에 비해 희귀한 단어의 번역에서 더 나은 정확도를 보이며, 훈련 시에 보지 못한 새로운 단어를 생성할 수 있는 능력을 갖추고 있음을 확인했습니다.\n",
        "Our analysis shows that the neural networks are able to learn compounding and transliteration from sub- word representations.\n",
        "\n",
        "우리의 분석 결과, 신경망은 서브워드 표현으로부터 합성 및 표음법을 학습할 수 있는 것으로 나타났다.\n",
        "\n",
        "\n",
        "We show that open-vocabulary neural machine translation is possible by encoding (rare) words via subword units. We find our architecture simpler and more effective than using large vocabularies and back-off dictionaries.\n",
        "\n",
        "우리는 서브워드 단위를 통해 (희귀한) 단어를 인코딩함으로써 개방 어휘 신경 기계 번역이 가능하다는 것을 보여줍니다. 우리는 이 아키텍처가 큰 어휘와 백오프 사전을 사용하는 것보다 간단하면서도 더 효과적인 것으로 발견했습니다.\n",
        "\n",
        "해석: 서브워드 방법의 NMT는 적은 어휘로도 효과적인 번역이 가능하다\n",
        "\n",
        "We adapt byte pair encoding (BPE) (Gage, 1994), a compression algorithm, to the task of word segmentation. BPE allows for the representation of an open vocabulary through a fixed-size vocabulary of variable-length character sequences, making it a very suitable word segmentation strategy for neural network models.\n",
        "\n",
        "우리는 압축 알고리즘인 byte pair encoding (BPE) (Gage, 1994)를 단어 분할 작업에 적용합니다. BPE는 가변 길이의 문자 시퀀스로 구성된 고정 크기 어휘를 통해 개방 어휘의 표현이 가능하게 해주어 신경망 모델에 매우 적합한 단어 분할 전략으로 만듭니다.\n",
        "\n",
        "\n",
        "NMT란?\n",
        "\n",
        "We follow the neural machine translation architecture by Bahdanau et al. (2015), which we will briefly summarize here. However, we note that our approach is not specific to this architecture.\n",
        "\n",
        "\n",
        "우리는 Bahdanau 등의 신경 기계 번역 아키텍처(2015)를 따르며, 여기에선 간단하게 요약할 것임. 그러나 우리의 접근 방식은 이 아키텍처에 특정된 것이 아님.\n",
        "\n",
        "The neural machine translation system is implemented as an encoder-decoder network with recurrent neural networks.\n",
        "\n",
        "NMT 시스템은 RNN과 인코더-디코더 구성이 함께 시행된다.\n",
        "\n",
        "The encoder is a bidirectional neural network with gated recurrent units\n",
        "\n",
        "인코더는 Bi-RNN이다. 양방향 RNN의 수식은 다음과 같다\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "본 논문에서는 t대신 j를 사용함.\n",
        "\n",
        "The decoder is a recurrent neural network that predicts a target sequence\n",
        "\n",
        "디코더는 RNN이다.\n",
        "\n",
        "\n",
        "RNN이란?\n",
        "\n",
        "순차 데이터 또는 시계열 데이터의 경우, 이전의 정보가 다음의 결과에 영향을 미치기 때문에, 기존의 피드 포워드 신경망만으로는 충분하지 않다. RNN은 이러한 시간에 따라 누적된 정보를 처리할 수 있는 신경망이다.\n",
        "\n",
        "피드 포워드 신경망(FNN)은 다층 퍼셉트론 예제할때 했던 것\n",
        "\n",
        "RNN은 메모리 셀을 가지고 있음 따라서 시간에 따라 정보를 저장하고, 이전의 정보를 이용해서 현재의 결과를 예측하거나 분류할 수 있다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Training is performed on a parallel corpus with stochastic gradient descent. For translation, a beam search with small beam size is employed\n",
        "\n",
        "\n",
        "훈련은 확률적 경사 하강법을 사용한 병렬 말뭉치에서 수행됩니다. 번역 시에는 작은 빔 크기를 사용한 빔 서치가 적용됩니다.\n",
        "\n",
        "빔서치는 가장 가능성이 높은 시퀀스를 찾는 역할을 함.\n",
        "\n",
        "Subword Translation\n",
        "\n",
        "The main motivation behind this paper is that the translation of some words is transparent in that they are translatable by a competent transla- tor even if they are novel to him or her, based on a translation of known subword units such as morphemes or phonemes. Word categories whose translation is potentially transparent include:\n",
        "\n",
        "\n",
        "이 논문의 주된 동기는 몇몇 단어들의 번역이 투명하다는 것입니다. 즉, 이러한 단어들은 알려지지 않은 단어라도 경험이 풍부한 번역가에 의해 번역될 수 있으며, 이는 형태소나 음운과 같은 알려진 서브워드 유닛의 번역을 기반으로 합니다. 번역이 잠재적으로 투명한 단어 범주에는 다음이 포함됩니다:\n",
        "\n",
        "해석: 모르는 단어도 문맥상으로 번역이 되듯 형태소나 음운 등의 서브워드로 번역이 가능하다. 그 예시는 다음과 같다.\n",
        "\n",
        "named entities. Between languages that share an alphabet, names can often be copied from source to target text. Transcription or transliteration may be required, especially if the alphabets or syllabaries differ. Example:\n",
        "Barack Obama (English; German)\n",
        "Барак Обама (Russian)\n",
        "バラク・オバマ (ba-ra-ku o-ba-ma) (Japanese)\n",
        "\n",
        "고유명사의 경우는 source를 target으로 그대로 복사한다. 다만 표기가 다른 언어의 경우 발음이나 표기가 추가된다.\n",
        "\n",
        "cognates and loanwords. Cognates and loan- words with a common origin can differ in regular ways between languages, so that character-level translation rules are sufficient (Tiedemann, 2012). Example:\n",
        "claustrophobia (English)\n",
        "Klaustrophobie (German)\n",
        "Клаустрофобия (Klaustrofobiâ) (Russian)\n",
        "\n",
        "동형어와 외래어는 발음의 차이 말고도 문자 수준의 번역 규칙만으로도 충분하다. 영어로 시작한 단어이기 때문에 형태가 비슷한 경우를 의미함\n",
        "\n",
        "\n",
        "morphologically complex words. Words con- taining multiple morphemes, for instance formed via compounding, affixation, or in- flection, may be translatable by translating the morphemes separately. Example:\n",
        "solar system (English)\n",
        "Sonnensystem (Sonne + System) (German)\n",
        "Naprendszer (Nap + Rendszer) (Hungarian)\n",
        "\n",
        "형태론적으로 복잡한 단어는 여러 형태소를 포함하는 단어임. 각 형태소를 개별적으로 번역함으로서 번역 가능함.\n",
        "\n",
        "\n",
        "In an analysis of 100 rare tokens (not among the 50000 most frequent types) in our German training data1, the majority of tokens are potentially translatable from English through smaller units.\n",
        "\n",
        "희귀한 토큰의 대부분이 영어를 통해 더 작은 단위를 통해 번역이 가능한 것으로 나타남.\n",
        "\n",
        "We find 56 compounds, 21 names, 6 loanwords with a common origin (emanci- pate→emanzipieren), 5 cases of transparent affix- ation (sweetish ‘sweet’ + ‘-ish’ → süßlich ‘süß’ + ‘-lich’), 1 number and 1 computer language iden- tifier.\n",
        "\n",
        "이러이러한 것들이었다.\n",
        "\n",
        "\n",
        "Our hypothesis is that a segmentation of rare words into appropriate subword units is sufficient to allow for the neural translation network to learn transparent translations, and to generalize this knowledge to translate and produce unseen words.2 We provide empirical support for this hypothesis in Sections 4 and 5. First, we discuss different subword representations.\n",
        "\n",
        "우리의 가설은 희귀한 단어를 적절한 서브워드 단위로 분할함으로써 신경 번역 네트워크가 투명한 번역을 학습하고 이 지식을 일반화하여 보이지 않는 단어를 번역하고 생성하는 데 충분하다는 것입니다. 이 가설에 대한 경험적인 지원을 4장과 5장에서 제시합니다. 먼저, 우리는 다양한 서브워드 표현에 대해 논의합니다.\n"
      ],
      "metadata": {
        "id": "mu5WWMZOmZzD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxyO5R67maVT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}